<html>

<head>
<title>Grid Integration</title>
<style>
pre { margin-left: 20pt; }
.red { color: red; }
.skip { margin-top: 10pt; }
</style>
</head>

<body>

<h1>Integrating services from the Language Grid and the LAPPS Grid</h1>

<p><i>This document resulted from discussions at the May 2015 Grid meeting at
Vassar College, attended by Takao Nakaguchi, Keith Suderman, Nancy Ide, Chunqi
Shi and Marc Verhagen. Notes by Marc Verhagen.</i></p>

<p>There are four Language Grid service types for which there can be a
meaningful exchange between the Language Grid and the LAPPS Grid:</p>

<ol>
<li>MorphologicalAnalysis</li>
<li>NamedEntityTagging</li>
<li>DependencyParser</li>
<li>ConceptDictionary</li>
</ol>

<p>For the second and third types it is obvious what LAPPS services we will be
mapping to and from. Note that for the first type we are talking about mapping a
single Language Grid service to a sequence of two LAPPS services, a tokenizer
and a tagger. The rest of this document discusses the issues in those three
mappings. The ConceptDictionary service may in the future map to LAPPS-LDC
dictionary services, but are excluded from the discussion here.</p>



<h2>1. MorphologicalAnalysis</h2>

<p>On the Language Grid end we have a MorphologicalAnalysisService (MA), for
example jp.go.langrid.service_1_2.morphologicalanalysis. As input it takes a
pair <"language", "text"> and as output it produces a list of lemmas, where each
lemma has a lemma, a word and a part of speech:</p>

<pre>
[ {"lemma", "word", "partOfSpeech"}, ... ]
</pre>

<p>This list is guaranteed to follow the token order as expressed in the
  text. But note that it may not be possible to exactly generate the input text
  from this list.</p>

<p>On the LAPPS end we have a tokenizer and a tagger, both accepting LIF JSON as
  input and output (in the following, the LIF format is liberally violated for
  brevity). The StanfordTokenizer, or any tokenizer, takes as input a structure
  with a discriminator and a payload, where the payload is an object in LIF
  format with a text and an empty list of views.</p>

<pre>
{
  "text": { 
	"@value": "Fido barks.",
	"@language": "en" },
  "views": []
}
</pre>

<p>The output has a view added:</p>

<pre>
{
  "text": { 
	"@value": "Fido barks.",
	"@language": "en" },
  "views": [
	{
	"metadata": {...}
	"annotations": [ 
	"annotations": [ 
	   { "@type": "Token", "id": "t0", "start": 0, "end": 4, 
		"features": { "word": "Fido" } },
	   { "@type": "Token", "id": "t1", "start": 5, "end": 10, 
		"features": { "word": "barks" } },
	   { "@type": "Token", "id": "t1", "start": 10, "end": 11, 
		"features": { "word": "." } } ]
	}
  ]
}
</pre>

<p>Note: the offical LIF specifications do currently not require there to be a
  "word" feature in the feature dictionary.</p>

<p>This object will be input to a tagger, for example the StanfordPosTagger,
  which will then add a view and produce</p>

<pre>
{
  "text": { 
	"@value": "Fido barks.",
	"@language": "en" },
  "views": [
	{
	"metadata": {...}
	"annotations": [ 
	   { "@type": "Token", "id": "t0", "start": 0, "end": 4, 
		"features": { "word": "Fido" } },
	   { "@type": "Token", "id": "t1", "start": 5, "end": 10, 
		"features": { "word": "barks" } },
	   { "@type": "Token", "id": "t1", "start": 10, "end": 11, 
		"features": { "word": "." } } ]
	}
	{
	"metadata": {...}
	"annotations": [ 
	   { "@type": "Token", "id": "t0", "start": 0, "end": 4, 
		"features": { "word": "Fido", "pos": "NNP" } },
	   { "@type": "Token", "id": "t1", "start": 5, "end": 10, 
		"features": { "word": "barks", "pos": "VBZ" } },
	   { "@type": "Token", "id": "t1", "start": 10, "end": 11, 
		"features": { "word": ".", "pos": "." } } ]
	}
  ]
}
</pre>

<p>There may or may not be a "lemma" feature, in the case of the wrapped
  Stanford tokenizer and tagger there is no such feature.</p>


<h3>Conversion</h3>

<p>The conversion for the input format is fairly straightforward. The input text
  and the language are independent variables on both ends. Converting the output
  is more involved.</p>


<h4>LAPPS Grid --> Language Grid</h4>

<p>This is fairly straightforward. First the list of annotations may need to be
ordered since LIF does not guarantee and ordered list (yet commonly does order
tokens). Pick out the word and pos features. Use the lemma feature if there
is one, otherwise use the word. If there is no word features, use the start and
end offsets to get the word from the text feature. The pos needs to be
translated to the Language Grid set of tags.</p>


<h4>Language Grid --> LAPPS Grid</h4>

<p>The main issues is that the MorphologicalAnalyzer does not create standoff so
we need a mapping from lemmas to text offsets. We can use Keith's alignment
program and/or the Brandeis alignment code. The former is already wrapped, the
latter needs to be dusted off. There may be issues when for example the MA would
take "The dog don't bite." and creates lemmas ["the" "dog" "does" "not" "bite"
"."]. We will aim to have alignment that gets most cases right, but reserve the
right to gracefully fail on parts of the input.</p>

<p>The Language Grid categories are a reduced set that is much smaller than the
typical POS sets on the LAPPS Grid. We will not attempt a mapping but simply use
the source set and specify this in the meta data fo the view. The LAPPS Vocab
will add the Language Grid tag set, for example at
vocab.lappsgrid.org/tagsets/laguage-grid</p>

<!--
Conversion - running Lapps for MA

Tokenizer -> MA  -  lack of lemma and pos
PosTagger -> MA  -  need word
Tok+PosTag -> MA
-->


<h2>2. NamedEntityTagging</h2>

<p>The Language Grid StanfordNE again takes a pair of language and text as
input. If morphological analysis is needed than it will be embedded in the
service. The output is a list of entity-tag pairs:</p>

<pre>[ { "entity", "tag" }, ... ]</pre>

<p>The entities basically line up with the lemmas that would be created by an
MA. All lemmas will be listed, not just the ones in entities, and the order
reflects the order in the text:</p>

<pre>
[
  { entity: 'John', tag: 'Person' },
  { entity: 'Smith', tag: 'Person' },
  { entity: 'sleeps', tag: 'MISC' }
]
</pre>

<p>This is basically an IO format (but not a BIO format so we have to guess that
'John Smith' is one NE which is usually the right guess). There is no type system
that specifies what tags (in addition to MISC) are allowed, the types depend on
the tool that is wrapped.</p>

<p>On the LAPPS Grid end, assuming we use the Stanford NE, the input would
include a view with tokens and tags as created by a tokenizer and a tagger. The
output has a view added with the named entities.</p>

<pre>
{
  "text": { 
	"@value": "John Smith sleeps",
	"@language": "en" },
  "views": [
	{
	  "metadata": {...}
	  "annotations": [ 
	      { "@type": "Token", "id": "t0", "start": 0, "end": 4, 
		"features": { "word": "Fido", "pos": "NNP" } },
	      { "@type": "Token", "id": "t1", "start": 5, "end": 10, 
		"features": { "word": "barks", "pos": "NNP" } },
	      { "@type": "Token", "id": "t1", "start": 10, "end": 16, 
		"features": { "word": ".", "pos": "." } } 
	   ]
	}
	{
	  "metadata": {...}
	  "annotations": [ 
	      { "@type": "Person", "id": "ne0", "start": "0", "end": "11" } 
	  ]
	}
  ]
}
</pre>

<p>NOTE. There are currently no LIF specifications for named entities so aspects
of this may change. However, NamedEntity, Person and some other entity types are
mentioned in the vocabulary.</p>


<!--
discriminator
payload
	views: 
		{ annotations: [
			{ id, start, end, label, features: { pos: NNP, word: 'john'} } ]
			NOTE: this is different from the vocab
			NOTE; no LIF specs for this
</pre>

Stanford NE in LAPPS and LANG:
1- returns all tokens in a NE as a NE (individual tokens are NEs)
2- what is the native output?
3- how is this output interpreted by wrappers
	LANG - tokens marked as being part of an NE
	LAPPS - entire span marked as one tag

Point 3 determines whether we can exchange NE components 
-> we can since there is a consistent, yet different, interpretation.
-->


<h3>Conversion</h3>

<p>There are three issues with conversion.</p>

<ol>

<li>As with mapping from an MA, there are no character offsets in the Language
  Grid service output. However, since the entities line up with lemma, the same
  alignment code as used for the MA can be used here.</li>

<li class="skip">The phrase "John Smith" is represented differently: "[John]
  [Smith]" versus "[John Smith]". For mapping to the LAPPS Grid format we can
  just group adjacent entities and for the opposite direction we split the
  phrase.</li>

<li class="skip">Listing all tokens versus listing only those in the named
  entity. This is no issue for the mapping to the LAPPS Grid format since it
  would just ignore the extra tokens (even though it would use them when
  calculating the offsets). When mapping to the Language Grid format we would
  need to reconstruct the full list of lemmas, which may be tricky if there is
  no LIF view with tokens.</li>

</ol>

<p>To elaborate a bit on the last point. The input for LAPPS NE services may be
  just text or text with a tokens view. For now we should perhaps only deal with
  the latter (that is, do not expose NE components that do not use the tokens;
  this makes integration easier because we do not have to worry about
  inconsistent tokenizations between LANG output and NEs that might be
  created.</p>


<!--
"John is tired, John is old"
	==> [John] vs [John] [John]
	mentions or types? ==> mentions

"I see John DOe, John is old"
	==> [John Doe] vs [John Doe] [John]
	normalization? ==> no

Input for LANG is text (language text)
-->




<h2>3. DependencyParser</h2>

<p>The dependency parsers on the Language Grid again take a text and a language
as input. The output they create is a list of chunks where each chunk has an
identifier, a dependency and a list of morphemes. Here are the first two chunks
for "Our son Isaac lifted..." as parsed by the Stanford dependency parser:</p>

<pre>
{
	"chunkId": "0",
	"dependency": {	"headChunkId": "1", "label": "DEPENDENCY" },
	"morphemes": [
		{ "lemma": "My", "partOfSpeech": "noun.pronoun", "word": "My" }	]
},
{
	"chunkId": "1",
	"dependency": { "headChunkId": "5", "label": "DEPENDENCY" },
	"morphemes": [
		{ "lemma": "son", "partOfSpeech": "noun.common", "word": "son" } ]
},
</pre>

<p>As with the other services, the order in the chunks and morphemes list
reflects the order in the text. The label always is DEPENDENCY. If there are
more sentences in the input, the output is still just one list of chunks. There
are no explicit sentence boundaries, but the root element of each sentence has
the headChunkID set to -1. For the Stanford dependency parser, the list of
morphemes always has one element. This is different for the Japanese
CabochaService, where the dependencies are not between individual lemmas.</p>

<p>Dependency parsers on the LAPPS Grid (should) produce the following output.</p>

<pre>
{
  "text": "Sue sees herself",
  "views": [
    { "metadata": {},
      "annotations": [
         { "@type": "Token", "id": "tok0", "start": 0, "end": 3 },
         { "@type": "Token", "id": "tok1", "start": 4, "end": 8 },
         { "@type": "Token", "id": "tok2", "start": 9, "end": 16 } ]
    },
    { "metadata": {}
      "annotations": [
         { "@type": "DependencyStructure",
           "id": "depstructure0",
           "start": 0,
           "end": 16,
           "features": {
              "type": "basic-dependencies",
              "dependencies": [
                 { "@type": "Dependency",
                   "label": "ROOT",
                   "id": "dep0",
                   "features":	 {
                     "governor": null,
                     "dependent": "tok1" }},
                 { "@type": "Dependency",
                   "label": "nsubj",
                   "id": "dep1",
                   "features":	 {
                     "governor": "tok1",
                     "dependent": "tok0" }},
                 { "@type": "Dependency",
                   "label": "nobj",
                   "id": "dep2",
                   "features":	 {
                     "governor": "tok1",
                     "dependent": "tok2" }}]}}]
    }
  ]
}
</pre>

<p>The main differences with the Language Grid dependency parse output are:</p>

<ol>

<li>There are no multi-token chunks, all nodes in the parse are single
tokens. But note that in the Language Grid format there are only multi-token
chunks for Japanese, for English there is no difference.</li>

<li>There are actual labels</li>

<li>There is one dependency structure per sentence</li>

<li>The links are embedded deeper in the strucutre</li>

<li>There are character offsets (indirectly)</li>

</ol>

<p>The mapping from the Language Grid format to the LAPPS Grid format is pretty
straightforward, assuming the allignment problem is solved. The main problem
going the other direction is to calculate the order of the chunks.</p>


<!--
Seems to be between sequences of lemmas and not between individual tokens

The label was always DEPENDENCY

Takao to send two examples, one on Japanese (CabochaService), one on English (Stanford)

Input for LANG is text (language text)

LANG: one list, root elements have headChunkID equal to -1
LAPPS: list of sentences, each has one with governor equal to null

LANG: all info in toplevel elements in the list
LAPPS: embedded in toplevel of dependencies feature inside of the DependencyStructure

LANG			LAPPS
chunkID			id
dependency:headChunkID	features:governor
dependency:label	label
morphemes		features:dependent

Mapping fairly straightforward.

BUT
	need to deal with offsets
	if more than one morpheme
		- create more than one dependency, or
		- allow list of identifers as value of features:dependent
-->


<h2>4. Final Notes</h2>

<p>We will create eight converters:</p>

<ul>
<li>Language Grid data <--> LAPPS Grid data
<li>MorphologicalAnalyzer output format <--> LIF with Token view
<li>NamedEntityTagging output format <--> LIF with NE view
<li>DependencyParser output format <--> LIF with token view and DependencyStructure view
</ul>


<p>LAPPS people to create the convertors to the LIF format<br/>
Language Grid people to create the converters to the LANG formats</p>

<p>LAPPS - will have all converters living locally, at workflow creation time
these will be used as needed, for now this may be the responsibility of the
user.</p>

<p>LAPPS - we will create a service that gets metadata from services, it
forwards to local LAPPS services or does some local lookup for LANG services</p>

<p>Available LappsGrid and LangGrid services will at first be hard-coded
somewhere.</p>

<p>A note on view meta data. When LAPPS gets Person types from LANG, we can use
http://vocab.lapps.org/language-grid/SERVICE_NAME/Person. We will have a lookup
list that may map this to Person, created manually. The converter will access
that list.</p>


<!--
FIX LIF for NE and Dependencies - look into that

FIX problems for NEs
	- now take each token as an NE
	- uppercase versus lower case (PERSON vs person)
	- TIME/DATE thing
		Gate uses one, Stanford the other
-->

</body>
</html>
